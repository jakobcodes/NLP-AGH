{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification reranking (7-9)\n",
    "### Jakub Łubkowski, Marcin Mikuła\n",
    "\n",
    "7. Use the classifier as a re-ranker for finding the answers to the questions. Since the re-ranker is slow, you\n",
    "   have to limit the subset of possible passages to top-n (10, 50 or 100 - depending on your GPU) texts returned by much faster model, e.g. FTS.\n",
    "8. The scheme for re-ranking is as follows:\n",
    "   - Find passage candidates using FTS, where the query is the question.\n",
    "   - Take top-n results returned by FTS.\n",
    "   - Use the model to classify all pairs, where the first sentence is the question (query) and the second sentence is\n",
    "      the passage returned by the FTS.\n",
    "   - Use the score returned by the model (i.e. the probability of the **positive** outcome) to re-rank the passages.\n",
    "9. Compute how much the result of searching the passages improved over the results from lab 2. Use NDCG to compare the\n",
    "   results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load data from /data/raw\n",
    "corpus_df = pd.read_csv('data/raw/corpus.csv')\n",
    "q_df = pd.read_csv('data/raw/queries.csv')\n",
    "qa_df = pd.read_csv('data/raw/qrels.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import elasticsearch\n",
    "\n",
    "es = elasticsearch.Elasticsearch(\"http://localhost:9200\")\n",
    "es.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "class BertReranker:\n",
    "    def __init__(self, model_path=\"model\", tokenizer_path=\"tokenizer\"):\n",
    "        if torch.backends.mps.is_available():\n",
    "            self.device = torch.device(\"mps\")\n",
    "        elif torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_path).to(self.device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "        self.model.eval()  # Set to evaluation mode\n",
    "\n",
    "    def get_score(self, query, passage):\n",
    "        # Prepare input\n",
    "        text_pair = f\"{query} {self.tokenizer.sep_token} {passage}\"\n",
    "        inputs = self.tokenizer(\n",
    "            text_pair,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Get prediction\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            probabilities = torch.softmax(outputs.logits, dim=1)\n",
    "            positive_score = probabilities[0][1].item()  # Probability of positive class\n",
    "        \n",
    "        return positive_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_ndcg_with_reranking(\n",
    "        qa_dataset, \n",
    "        queries_dataset, \n",
    "        es, \n",
    "        index_name, \n",
    "        field, \n",
    "        reranker=None, \n",
    "        top_n=10\n",
    "    ):\n",
    "    ndcg_scores = []\n",
    "    \n",
    "    # Convert datasets to DataFrames if they aren't already\n",
    "    qa_df = pd.DataFrame(qa_dataset) if not isinstance(qa_dataset, pd.DataFrame) else qa_dataset\n",
    "    queries_df = pd.DataFrame(queries_dataset) if not isinstance(queries_dataset, pd.DataFrame) else queries_dataset\n",
    "    \n",
    "    for query_id in qa_df['query-id'].unique():\n",
    "        # Find the query text\n",
    "        query_row = queries_df[queries_df['_id'] == query_id]\n",
    "        if query_row.empty:\n",
    "            print(f\"Warning: No query found for query_id {query_id}\")\n",
    "            continue\n",
    "        \n",
    "        query = query_row.iloc[0]['text_query']\n",
    "        corpus_ids = qa_df[qa_df['query-id'] == query_id]['corpus-id']\n",
    "        corpus_ids_set = set(int(id) for id in corpus_ids)\n",
    "        \n",
    "        # Perform the search\n",
    "        try:\n",
    "            search_results = es.search(\n",
    "                index=index_name,\n",
    "                body={\n",
    "                    \"query\": {\n",
    "                        \"match\": {\n",
    "                            field: query\n",
    "                        }\n",
    "                    },\n",
    "                    \"size\": top_n  # Get top_n results for reranking\n",
    "                }\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error performing search for query_id {query_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "        hits = search_results['hits']['hits']\n",
    "        \n",
    "        if reranker:\n",
    "            # Rerank the results\n",
    "            reranked_hits = []\n",
    "            for hit in hits:\n",
    "                passage = hit['_source'][field]\n",
    "                score = reranker.get_score(query, passage)\n",
    "                reranked_hits.append((hit, score))\n",
    "            \n",
    "            # Sort by reranker score\n",
    "            reranked_hits.sort(key=lambda x: x[1], reverse=True)\n",
    "            hits = [hit[0] for hit in reranked_hits]\n",
    "\n",
    "        # Extract document IDs from final results\n",
    "        retrieved_ids = [int(hit['_source']['corpus_id']) for hit in hits]\n",
    "\n",
    "        # Create relevance scores for retrieved documents\n",
    "        relevance_scores = [1 if doc_id in corpus_ids_set else 0 for doc_id in retrieved_ids]\n",
    "\n",
    "        # Create true relevance scores\n",
    "        true_relevance = [1] * len(corpus_ids)\n",
    "\n",
    "        # Pad both lists to ensure they have exactly 5 elements\n",
    "        relevance_scores = (relevance_scores + [0] * 5)[:5]\n",
    "        true_relevance = (true_relevance + [0] * 5)[:5]\n",
    "\n",
    "        # Calculate NDCG@5\n",
    "        ndcg = ndcg_score([true_relevance], [relevance_scores], k=5)\n",
    "        ndcg_scores.append(ndcg)\n",
    "    \n",
    "    return sum(ndcg_scores) / len(ndcg_scores) if ndcg_scores else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"fiqa_pl_corpus\"\n",
    "\n",
    "# Initialize reranker\n",
    "reranker = BertReranker()\n",
    "\n",
    "# Test setups\n",
    "setups = [\n",
    "    (\"No synonyms, No lemmatization\", \"text_without_synonyms_no_lemma\"),\n",
    "    (\"Synonyms, No lemmatization\", \"text_with_synonyms_no_lemma\"),\n",
    "    (\"No synonyms, Lemmatization\", \"text_without_synonyms_with_lemma\"),\n",
    "    (\"Synonyms, Lemmatization\", \"text_with_synonyms_with_lemma\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NDCG@5 scores without reranking:\n",
      "No synonyms, No lemmatization:\n",
      "NDCG@5: 0.7637\n",
      "\n",
      "Synonyms, No lemmatization:\n",
      "NDCG@5: 0.7637\n",
      "\n",
      "No synonyms, Lemmatization:\n",
      "NDCG@5: 0.7683\n",
      "\n",
      "Synonyms, Lemmatization:\n",
      "NDCG@5: 0.7683\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNDCG@5 scores without reranking:\")\n",
    "for setup_name, field in setups:\n",
    "    print(f\"{setup_name}:\")\n",
    "    avg_ndcg_score = calculate_ndcg_with_reranking(\n",
    "        qa_df, q_df, es, index_name, field, reranker=None\n",
    "    )\n",
    "    print(f\"NDCG@5: {avg_ndcg_score:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NDCG@5 scores with reranking:\n",
      "No synonyms, No lemmatization:\n",
      "NDCG@5: 0.7691\n",
      "\n",
      "Synonyms, No lemmatization:\n",
      "NDCG@5: 0.7691\n",
      "\n",
      "No synonyms, Lemmatization:\n",
      "NDCG@5: 0.7785\n",
      "\n",
      "Synonyms, Lemmatization:\n",
      "NDCG@5: 0.7785\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNDCG@5 scores with reranking:\")\n",
    "for setup_name, field in setups:\n",
    "    print(f\"{setup_name}:\")\n",
    "    avg_ndcg_score = calculate_ndcg_with_reranking(\n",
    "        qa_df, q_df, es, index_name, field, reranker=reranker, top_n=3\n",
    "    )\n",
    "    print(f\"NDCG@5: {avg_ndcg_score:.4f}\")\n",
    "    print() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Do you think simpler methods, like Bayesian bag-of-words model, would work for sentence-pair classification? Justify your answer.\n",
    "\n",
    "A Bayesian bag-of-words model would likely be insufficient for sentence-pair classification because:\n",
    "\n",
    "1. It loses word order and context, which are crucial for understanding relationships between sentences\n",
    "2. It can't effectively capture semantic relationships between question-answer pairs\n",
    "3. It struggles with paraphrasing and synonyms, which are common in Q&A scenarios\n",
    "\n",
    "While simpler and faster to train, it would likely perform worse than our transformer model which achieved NDCG scores of ~0.77 by understanding deeper semantic relationships.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What hyper-parameters you have selected for the training? What resources (papers, tutorial) you have consulted to select these hyper-parameters?\n",
    "\n",
    "Selected hyperparameters:\n",
    "- Model: bert-base-polish-uncased-v1 (Polish BERT base model)\n",
    "- Batch size: 16 (train and eval)\n",
    "- Number of epochs: 3\n",
    "- Learning rate: Dynamic with warmup\n",
    "- Warmup steps: 500\n",
    "- Weight decay: 0.01\n",
    "- Evaluation strategy: Steps-based (every 500 steps)\n",
    "- Max sequence length: 512 tokens\n",
    "\n",
    "Resources consulted:\n",
    "1. \"Fine-tuning BERT for Text Classification\" tutorial from Huggingface\n",
    "2. Polish BERT model documentation (dkleczek/bert-base-polish-uncased-v1)\n",
    "\n",
    "The parameters were chosen based on common best practices for BERT fine-tuning, with batch size and learning rate adjusted for our hardware constraints and dataset size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Think about pros and cons of the neural-network models with respect to natural language processing. Provide at least 2 pros and 2 cons.\n",
    "\n",
    "Pros:\n",
    "1. Semantic Understanding: Good at capturing complex relationships in text\n",
    "2. Adaptability: Can handle variations in language (synonyms, paraphrasing) well\n",
    "\n",
    "Cons:\n",
    "1. Resource Heavy: Requires significant computational power and memory\n",
    "2. Lack of Interpretability: Hard to understand why the model makes specific decisions, unlike rule-based systems\n",
    "3. Overfitting Risk: Tend to overfit on small datasets, requiring large amounts of training data for reliable performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
