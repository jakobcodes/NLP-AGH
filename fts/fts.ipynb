{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48444448-735f-4df4-b2ed-fd1cd4a5282f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Elasticsearch\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import json\n",
    "\n",
    "# Connect to Elasticsearch\n",
    "es = Elasticsearch([\"http://localhost:9200\"])\n",
    "\n",
    "# Check if the connection was successful\n",
    "if es.ping():\n",
    "    print(\"Connected to Elasticsearch\")\n",
    "else:\n",
    "    print(\"Could not connect to Elasticsearch\")\n",
    "    exit()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d971a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document indexed: created\n",
      "\n",
      "Search Results:\n",
      "ID: -qIPyJIB8Q5--6byl4v_\n",
      "Score: 0.03922071\n",
      "Source: {\n",
      "  \"title\": \"Example Document\",\n",
      "  \"content\": \"This is a sample document for Elasticsearch.\",\n",
      "  \"tags\": [\n",
      "    \"example\",\n",
      "    \"demo\",\n",
      "    \"elasticsearch\"\n",
      "  ]\n",
      "}\n",
      "---\n",
      "ID: -6I8yJIB8Q5--6byIYsu\n",
      "Score: 0.03922071\n",
      "Source: {\n",
      "  \"title\": \"Example Document\",\n",
      "  \"content\": \"This is a sample document for Elasticsearch.\",\n",
      "  \"tags\": [\n",
      "    \"example\",\n",
      "    \"demo\",\n",
      "    \"elasticsearch\"\n",
      "  ]\n",
      "}\n",
      "---\n",
      "ID: _KJAyJIB8Q5--6byy4uW\n",
      "Score: 0.03922071\n",
      "Source: {\n",
      "  \"title\": \"Example Document\",\n",
      "  \"content\": \"This is a sample document for Elasticsearch.\",\n",
      "  \"tags\": [\n",
      "    \"example\",\n",
      "    \"demo\",\n",
      "    \"elasticsearch\"\n",
      "  ]\n",
      "}\n",
      "---\n",
      "ID: I6NYyJIB8Q5--6bycG1a\n",
      "Score: 0.03922071\n",
      "Source: {\n",
      "  \"title\": \"Example Document\",\n",
      "  \"content\": \"This is a sample document for Elasticsearch.\",\n",
      "  \"tags\": [\n",
      "    \"example\",\n",
      "    \"demo\",\n",
      "    \"elasticsearch\"\n",
      "  ]\n",
      "}\n",
      "---\n",
      "ID: eqRnyJIB8Q5--6byScOp\n",
      "Score: 0.03922071\n",
      "Source: {\n",
      "  \"title\": \"Example Document\",\n",
      "  \"content\": \"This is a sample document for Elasticsearch.\",\n",
      "  \"tags\": [\n",
      "    \"example\",\n",
      "    \"demo\",\n",
      "    \"elasticsearch\"\n",
      "  ]\n",
      "}\n",
      "---\n",
      "ID: onC4zJIBfWT7GLHH2-7C\n",
      "Score: 0.03922071\n",
      "Source: {\n",
      "  \"title\": \"Example Document\",\n",
      "  \"content\": \"This is a sample document for Elasticsearch.\",\n",
      "  \"tags\": [\n",
      "    \"example\",\n",
      "    \"demo\",\n",
      "    \"elasticsearch\"\n",
      "  ]\n",
      "}\n",
      "---\n",
      "ID: RK6cxJIBL6izZLNZqNzH\n",
      "Score: 0.03922071\n",
      "Source: {\n",
      "  \"title\": \"Example Document\",\n",
      "  \"content\": \"This is a sample document for Elasticsearch.\",\n",
      "  \"tags\": [\n",
      "    \"example\",\n",
      "    \"demo\",\n",
      "    \"elasticsearch\"\n",
      "  ]\n",
      "}\n",
      "---\n",
      "ID: QiC3xJIBYTf5ZIOG0QkG\n",
      "Score: 0.03922071\n",
      "Source: {\n",
      "  \"title\": \"Example Document\",\n",
      "  \"content\": \"This is a sample document for Elasticsearch.\",\n",
      "  \"tags\": [\n",
      "    \"example\",\n",
      "    \"demo\",\n",
      "    \"elasticsearch\"\n",
      "  ]\n",
      "}\n",
      "---\n",
      "ID: QSCzxJIBYTf5ZIOGMAkh\n",
      "Score: 0.03922071\n",
      "Source: {\n",
      "  \"title\": \"Example Document\",\n",
      "  \"content\": \"This is a sample document for Elasticsearch.\",\n",
      "  \"tags\": [\n",
      "    \"example\",\n",
      "    \"demo\",\n",
      "    \"elasticsearch\"\n",
      "  ]\n",
      "}\n",
      "---\n",
      "ID: Q66cxJIBL6izZLNZlNyV\n",
      "Score: 0.03922071\n",
      "Source: {\n",
      "  \"title\": \"Example Document\",\n",
      "  \"content\": \"This is a sample document for Elasticsearch.\",\n",
      "  \"tags\": [\n",
      "    \"example\",\n",
      "    \"demo\",\n",
      "    \"elasticsearch\"\n",
      "  ]\n",
      "}\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Define a sample document\n",
    "document = {\n",
    "    \"title\": \"Example Document\",\n",
    "    \"content\": \"This is a sample document for Elasticsearch.\",\n",
    "    \"tags\": [\"example\", \"demo\", \"elasticsearch\"]\n",
    "}\n",
    "\n",
    "# Create an index (if it doesn't exist)\n",
    "index_name = \"demo_index\"\n",
    "if not es.indices.exists(index=index_name):\n",
    "    es.indices.create(index=index_name)\n",
    "    print(f\"Created index: {index_name}\")\n",
    "\n",
    "# Index the document\n",
    "response = es.index(index=index_name, body=document)\n",
    "print(f\"Document indexed: {response['result']}\")\n",
    "\n",
    "# Refresh the index to make the document searchable immediately\n",
    "es.indices.refresh(index=index_name)\n",
    "\n",
    "# Search for documents\n",
    "search_query = {\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            \"content\": \"sample\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "search_results = es.search(index=index_name, body=search_query)\n",
    "\n",
    "# Print search results\n",
    "print(\"\\nSearch Results:\")\n",
    "for hit in search_results['hits']['hits']:\n",
    "    print(f\"ID: {hit['_id']}\")\n",
    "    print(f\"Score: {hit['_score']}\")\n",
    "    print(f\"Source: {json.dumps(hit['_source'], indent=2)}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0aef84d8-64fa-4790-9db5-bacd62ec6389",
   "metadata": {},
   "outputs": [],
   "source": [
    "polish_analyzer = {\n",
    "    \"analysis\": {\n",
    "        \"filter\": {\n",
    "            \"polish_month_synonyms\": {\n",
    "                \"type\": \"synonym\",\n",
    "                \"synonyms\": [\n",
    "                    \"styczeń, sty, I\",\n",
    "                    \"luty, lut, II\",\n",
    "                    \"marzec, mar, III\",\n",
    "                    \"kwiecień, kwi, IV\",\n",
    "                    \"maj, V\",\n",
    "                    \"czerwiec, cze, VI\",\n",
    "                    \"lipiec, lip, VII\",\n",
    "                    \"sierpień, sie, VIII\",\n",
    "                    \"wrzesień, wrz, IX\",\n",
    "                    \"październik, paź, X\",\n",
    "                    \"listopad, lis, XI\",\n",
    "                    \"grudzień, gru, XII\"\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"analyzer\": {\n",
    "            \"polish_custom\": {  \n",
    "                \"type\": \"custom\",\n",
    "                \"tokenizer\": \"standard\",\n",
    "                \"filter\": [\n",
    "                    \"polish_month_synonyms\",\n",
    "                    \"lowercase\",\n",
    "                    \"morfologik_stem\"\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cdbb472-f177-4c72-91df-e5460337450e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a basic Polish analyzer without synonyms\n",
    "basic_polish_analyzer = {\n",
    "    \"analysis\": {\n",
    "        \"analyzer\": {\n",
    "            \"polish_basic\": {\n",
    "                \"type\": \"custom\",\n",
    "                \"tokenizer\": \"standard\",\n",
    "                \"filter\": [\n",
    "                    \"lowercase\",\n",
    "                    \"morfologik_stem\"\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebe8f8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated index fiqa_pl_corpus with new analyzers and mappings\n"
     ]
    }
   ],
   "source": [
    "fiqa_pl_index_mapping = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"text_with_synonyms\": {\"type\": \"text\", \"analyzer\": \"polish_custom\"},\n",
    "            \"text_without_synonyms\": {\"type\": \"text\", \"analyzer\": \"polish_basic\"}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "fiqa_pl_index_settings = {\n",
    "    \"settings\": {\n",
    "        \"analysis\": {\n",
    "            \"filter\": polish_analyzer[\"analysis\"][\"filter\"],\n",
    "            \"analyzer\": {\n",
    "                **polish_analyzer[\"analysis\"][\"analyzer\"],\n",
    "                **basic_polish_analyzer[\"analysis\"][\"analyzer\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": fiqa_pl_index_mapping[\"mappings\"]\n",
    "}\n",
    "\n",
    "\n",
    "# Create or update the index with the new settings and mapping\n",
    "fiqa_pl_index_name = \"fiqa_pl_corpus\"\n",
    "if es.indices.exists(index=fiqa_pl_index_name):\n",
    "    es.indices.close(index=fiqa_pl_index_name)\n",
    "    es.indices.put_settings(index=fiqa_pl_index_name, body=fiqa_pl_index_settings[\"settings\"])\n",
    "    es.indices.open(index=fiqa_pl_index_name)\n",
    "    print(f\"Updated index {fiqa_pl_index_name} with new analyzers and mappings\")\n",
    "else:\n",
    "    es.indices.create(index=fiqa_pl_index_name, body=fiqa_pl_index_settings)\n",
    "    print(f\"Created index {fiqa_pl_index_name} with custom Polish analyzers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfa31a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the FiQA-PL corpus dataset\n",
    "dataset = load_dataset('clarin-knext/fiqa-pl', 'corpus')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc07c9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'persistent': {}, 'transient': {}}\n"
     ]
    }
   ],
   "source": [
    "es = Elasticsearch([\"http://localhost:9200\"])\n",
    "\n",
    "# Get all settings\n",
    "all_settings = es.cluster.get_settings()\n",
    "print(all_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a7c9abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during indexing: Connection timed out\n",
      "Finished indexing FiQA-PL corpus to fiqa_pl_corpus\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from elasticsearch.helpers import bulk\n",
    "\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "\n",
    "# Function to generate actions for bulk indexing\n",
    "def generate_actions(documents):\n",
    "    for doc in documents:\n",
    "        yield {\n",
    "            \"_index\": fiqa_pl_index_name,\n",
    "            \"_source\": {\n",
    "                \"title\": doc[\"title\"],\n",
    "                \"text_with_synonyms\": doc[\"text\"],\n",
    "                \"text_without_synonyms\": doc[\"text\"]\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Bulk indexing function\n",
    "def bulk_index_documents(documents):\n",
    "    try:\n",
    "        success, failed = bulk(es, generate_actions(documents))\n",
    "        print(f\"Indexed {success} documents, {failed} failed\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during indexing: {e}\")\n",
    "\n",
    "\n",
    "# Perform bulk indexing\n",
    "bulk_index_documents(df.corpus)\n",
    "\n",
    "# Refresh the index\n",
    "es.indices.refresh(index=fiqa_pl_index_name)\n",
    "\n",
    "print(f\"Finished indexing FiQA-PL corpus to {fiqa_pl_index_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c890ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of documents in the index: 115776\n",
      "Number of matches for 'kwiecień' (without synonyms): 262\n",
      "Number of matches for 'kwiecień' (including synonyms): 311\n",
      "Additional matches found using synonyms: 49\n"
     ]
    }
   ],
   "source": [
    "# Get total number of documents in the index\n",
    "total_docs = es.count(index=fiqa_pl_index_name)['count']\n",
    "print(f\"Total number of documents in the index: {total_docs}\")\n",
    "\n",
    "# Search for 'kwiecień' without synonyms\n",
    "query_without_synonyms = {\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            \"text_without_synonyms\": \"kwiecień\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "result_without_synonyms = es.search(index=fiqa_pl_index_name, body=query_without_synonyms)\n",
    "matches_without_synonyms = result_without_synonyms['hits']['total']['value']\n",
    "print(f\"Number of matches for 'kwiecień' (without synonyms): {matches_without_synonyms}\")\n",
    "\n",
    "# Search for 'kwiecień' with synonyms\n",
    "query_with_synonyms = {\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            \"text_with_synonyms\": \"kwiecień\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "result_with_synonyms = es.search(index=fiqa_pl_index_name, body=query_with_synonyms)\n",
    "matches_with_synonyms = result_with_synonyms['hits']['total']['value']\n",
    "print(f\"Number of matches for 'kwiecień' (including synonyms): {matches_with_synonyms}\")\n",
    "\n",
    "# Calculate the difference\n",
    "difference = matches_with_synonyms - matches_without_synonyms\n",
    "print(f\"Additional matches found using synonyms: {difference}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cf512db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "queries_dataset = load_dataset(\"clarin-knext/fiqa-pl\", \"queries\")\n",
    "qa_pairs_dataset = load_dataset(\"clarin-knext/fiqa-pl-qrels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2251606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    queries: Dataset({\n",
      "        features: ['_id', 'title', 'text'],\n",
      "        num_rows: 6648\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['query-id', 'corpus-id', 'score'],\n",
      "        num_rows: 14166\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['query-id', 'corpus-id', 'score'],\n",
      "        num_rows: 1238\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['query-id', 'corpus-id', 'score'],\n",
      "        num_rows: 1706\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(queries_dataset)\n",
    "print(qa_pairs_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "effe043d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated index fiqa_pl_corpus with new analyzers and mappings\n"
     ]
    }
   ],
   "source": [
    "# Define analyzers with and without lemmatization and synonyms\n",
    "analyzers = {\n",
    "    \"analysis\": {\n",
    "        \"filter\": {\n",
    "            \"polish_month_synonyms\": {\n",
    "                \"type\": \"synonym\",\n",
    "                \"synonyms\": [\n",
    "                    \"styczeń, sty, I\",\n",
    "                    \"luty, lut, II\",\n",
    "                    \"marzec, mar, III\",\n",
    "                    \"kwiecień, kwi, IV\",\n",
    "                    \"maj, V\",\n",
    "                    \"czerwiec, cze, VI\",\n",
    "                    \"lipiec, lip, VII\",\n",
    "                    \"sierpień, sie, VIII\",\n",
    "                    \"wrzesień, wrz, IX\",\n",
    "                    \"październik, paź, X\",\n",
    "                    \"listopad, lis, XI\",\n",
    "                    \"grudzień, gru, XII\"\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"analyzer\": {\n",
    "            \"polish_custom_with_lemma\": {  \n",
    "                \"type\": \"custom\",\n",
    "                \"tokenizer\": \"standard\",\n",
    "                \"filter\": [\n",
    "                    \"polish_month_synonyms\",\n",
    "                    \"lowercase\",\n",
    "                    \"morfologik_stem\"\n",
    "                ]\n",
    "            },\n",
    "            \"polish_basic_with_lemma\": {\n",
    "                \"type\": \"custom\",\n",
    "                \"tokenizer\": \"standard\",\n",
    "                \"filter\": [\n",
    "                    \"lowercase\",\n",
    "                    \"morfologik_stem\"\n",
    "                ]\n",
    "            },\n",
    "            \"polish_custom_no_lemma\": {  \n",
    "                \"type\": \"custom\",\n",
    "                \"tokenizer\": \"standard\",\n",
    "                \"filter\": [\n",
    "                    \"polish_month_synonyms\",\n",
    "                    \"lowercase\"\n",
    "                ]\n",
    "            },\n",
    "            \"polish_basic_no_lemma\": {\n",
    "                \"type\": \"custom\",\n",
    "                \"tokenizer\": \"standard\",\n",
    "                \"filter\": [\n",
    "                    \"lowercase\"\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Update the index mapping\n",
    "fiqa_pl_index_mapping = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"text_with_synonyms_with_lemma\": {\"type\": \"text\", \"analyzer\": \"polish_custom_with_lemma\"},\n",
    "            \"text_without_synonyms_with_lemma\": {\"type\": \"text\", \"analyzer\": \"polish_basic_with_lemma\"},\n",
    "            \"text_with_synonyms_no_lemma\": {\"type\": \"text\", \"analyzer\": \"polish_custom_no_lemma\"},\n",
    "            \"text_without_synonyms_no_lemma\": {\"type\": \"text\", \"analyzer\": \"polish_basic_no_lemma\"}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "fiqa_pl_index_settings = {\n",
    "    \"settings\": {\n",
    "        \"analysis\": analyzers[\"analysis\"]\n",
    "    },\n",
    "    \"mappings\": fiqa_pl_index_mapping[\"mappings\"]\n",
    "}\n",
    "\n",
    "# Create or update the index with the new settings and mapping\n",
    "fiqa_pl_index_name = \"fiqa_pl_corpus\"\n",
    "if es.indices.exists(index=fiqa_pl_index_name):\n",
    "    es.indices.close(index=fiqa_pl_index_name)\n",
    "    es.indices.put_settings(index=fiqa_pl_index_name, body=fiqa_pl_index_settings[\"settings\"])\n",
    "    es.indices.put_mapping(index=fiqa_pl_index_name, body=fiqa_pl_index_settings[\"mappings\"])\n",
    "    es.indices.open(index=fiqa_pl_index_name)\n",
    "    print(f\"Updated index {fiqa_pl_index_name} with new analyzers and mappings\")\n",
    "else:\n",
    "    es.indices.create(index=fiqa_pl_index_name, body=fiqa_pl_index_settings)\n",
    "    print(f\"Created index {fiqa_pl_index_name} with custom Polish analyzers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ccec63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 57638 documents, [] failed\n",
      "Finished indexing FiQA-PL corpus to fiqa_pl_corpus\n"
     ]
    }
   ],
   "source": [
    "def generate_actions(documents):\n",
    "    for doc in documents:\n",
    "        yield {\n",
    "            \"_index\": fiqa_pl_index_name,\n",
    "            \"_source\": {\n",
    "                \"corpus_id\": str(doc[\"_id\"]),  # Include the original corpus ID in the source\n",
    "                \"title\": doc[\"title\"],\n",
    "                \"text_with_synonyms_with_lemma\": doc[\"text\"],\n",
    "                \"text_without_synonyms_with_lemma\": doc[\"text\"],\n",
    "                \"text_with_synonyms_no_lemma\": doc[\"text\"],\n",
    "                \"text_without_synonyms_no_lemma\": doc[\"text\"]\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Perform bulk indexing\n",
    "bulk_index_documents(df.corpus)\n",
    "\n",
    "# Refresh the index\n",
    "es.indices.refresh(index=fiqa_pl_index_name)\n",
    "\n",
    "print(f\"Finished indexing FiQA-PL corpus to {fiqa_pl_index_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5479c892",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_test_dataset = qa_pairs_dataset['test']\n",
    "\n",
    "# info about dataset\n",
    "# query_ids = qa_dataset['query-id']\n",
    "# corpus_ids = qa_dataset['corpus-id']\n",
    "# scores = qa_dataset['score']\n",
    "\n",
    "# Define setups\n",
    "setups = [\n",
    "    (\"No synonyms, No lemmatization\", \"text_without_synonyms_no_lemma\"),\n",
    "    (\"Synonyms, No lemmatization\", \"text_with_synonyms_no_lemma\"),\n",
    "    (\"No synonyms, Lemmatization\", \"text_without_synonyms_with_lemma\"),\n",
    "    (\"Synonyms, Lemmatization\", \"text_with_synonyms_with_lemma\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "920107a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>Co jest uważane za wydatek służbowy w podróży ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>Wydatki służbowe - ubezpieczenie samochodu pod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "      <td>Rozpoczęcie nowego biznesu online</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td></td>\n",
       "      <td>„Dzień roboczy” i „termin płatności” rachunków</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td></td>\n",
       "      <td>Nowy właściciel firmy – Jak działają podatki d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6643</th>\n",
       "      <td>4102</td>\n",
       "      <td></td>\n",
       "      <td>Jak mogę ustalić, czy moja stopa zwrotu jest „...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6644</th>\n",
       "      <td>3566</td>\n",
       "      <td></td>\n",
       "      <td>Gdzie mogę kupić akcje, jeśli chcę zainwestowa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6645</th>\n",
       "      <td>94</td>\n",
       "      <td></td>\n",
       "      <td>Wykorzystywanie punktów kart kredytowych do op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6646</th>\n",
       "      <td>2551</td>\n",
       "      <td></td>\n",
       "      <td>Jak znaleźć tańszą alternatywę dla tradycyjnej...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6647</th>\n",
       "      <td>2399</td>\n",
       "      <td></td>\n",
       "      <td>Skąd strony internetowe uzyskują informacje o ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6648 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       _id title                                               text\n",
       "0        0        Co jest uważane za wydatek służbowy w podróży ...\n",
       "1        4        Wydatki służbowe - ubezpieczenie samochodu pod...\n",
       "2        5                        Rozpoczęcie nowego biznesu online\n",
       "3        6           „Dzień roboczy” i „termin płatności” rachunków\n",
       "4        7        Nowy właściciel firmy – Jak działają podatki d...\n",
       "...    ...   ...                                                ...\n",
       "6643  4102        Jak mogę ustalić, czy moja stopa zwrotu jest „...\n",
       "6644  3566        Gdzie mogę kupić akcje, jeśli chcę zainwestowa...\n",
       "6645    94        Wykorzystywanie punktów kart kredytowych do op...\n",
       "6646  2551        Jak znaleźć tańszą alternatywę dla tradycyjnej...\n",
       "6647  2399        Skąd strony internetowe uzyskują informacje o ...\n",
       "\n",
       "[6648 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_test_df = pd.DataFrame(qa_test_dataset)\n",
    "queries_df = pd.DataFrame(queries_dataset['queries'])\n",
    "\n",
    "queries_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66094e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG@5 scores:\n",
      "No synonyms, No lemmatization:\n",
      "NDCG Sum: 494.8943947704632\n",
      "NDCG@5: 0.7637\n",
      "\n",
      "Synonyms, No lemmatization:\n",
      "NDCG Sum: 494.8943947704632\n",
      "NDCG@5: 0.7637\n",
      "\n",
      "No synonyms, Lemmatization:\n",
      "NDCG Sum: 497.86244389481055\n",
      "NDCG@5: 0.7683\n",
      "\n",
      "Synonyms, Lemmatization:\n",
      "NDCG Sum: 497.86244389481055\n",
      "NDCG@5: 0.7683\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "def calculate_ndcg(qa_dataset, queries_dataset, es, index_name, field):\n",
    "    ndcg_scores = []\n",
    "    \n",
    "    # Convert datasets to DataFrames if they aren't already\n",
    "    qa_df = pd.DataFrame(qa_dataset) if not isinstance(qa_dataset, pd.DataFrame) else qa_dataset\n",
    "    queries_df = pd.DataFrame(queries_dataset) if not isinstance(queries_dataset, pd.DataFrame) else queries_dataset\n",
    "    \n",
    "    for query_id in qa_df['query-id'].unique():\n",
    "        # Find the query text, with error handling if not found\n",
    "        query_row = queries_df[queries_df['_id'] == str(query_id)]\n",
    "        if query_row.empty:\n",
    "            print(f\"Warning: No query found for query_id {query_id}\")\n",
    "            continue\n",
    "        \n",
    "        query = query_row.iloc[0]['text']\n",
    "        corpus_ids = qa_df[qa_df['query-id'] == query_id]['corpus-id']\n",
    "\n",
    "        # Parse corpus_ids to a simple set of integers\n",
    "        corpus_ids_set = set(int(id) for id in corpus_ids)\n",
    "        \n",
    "        # Perform the search\n",
    "        try:\n",
    "            search_results = es.search(\n",
    "                index=index_name,\n",
    "                body={\n",
    "                    \"query\": {\n",
    "                        \"match\": {\n",
    "                            field: query\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error performing search for query_id {query_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Extract the document IDs from the search results\n",
    "        retrieved_ids = [int(hit['_source']['corpus_id']) for hit in search_results['hits']['hits']]\n",
    "\n",
    "        # Create relevance scores for retrieved documents\n",
    "        relevance_scores = [1 if doc_id in corpus_ids_set else 0 for doc_id in retrieved_ids]\n",
    "\n",
    "        # Create true relevance scores for all relevant documents\n",
    "        true_relevance = [1] * len(corpus_ids)\n",
    "\n",
    "        # Pad both lists to ensure they have exactly 5 elements\n",
    "        relevance_scores = (relevance_scores + [0] * 5)[:5]\n",
    "        true_relevance = (true_relevance + [0] * 5)[:5]\n",
    "\n",
    "        # Calculate NDCG@5\n",
    "        ndcg = ndcg_score([true_relevance], [relevance_scores], k=5)\n",
    "        ndcg_scores.append(ndcg)\n",
    "    \n",
    "    # Return the average NDCG@5 score\n",
    "    print('NDCG Sum: ' + str(sum(ndcg_scores)))\n",
    "    return sum(ndcg_scores) / len(ndcg_scores) if ndcg_scores else 0.0\n",
    "\n",
    "    \n",
    "\n",
    "print(\"NDCG@5 scores:\")\n",
    "for setup_name, field in setups:\n",
    "    print(f\"{setup_name}:\")\n",
    "    avg_ndcg_score = calculate_ndcg(qa_test_df, queries_df, es, fiqa_pl_index_name, field)\n",
    "    print(f\"NDCG@5: {avg_ndcg_score:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac21e57c",
   "metadata": {},
   "source": [
    "### 1. What are the strengths and weaknesses of regular expressions versus full text search regarding processing of text?\n",
    "\n",
    "Regular expressions offer precise pattern matching and efficient text manipulation, making them ideal for specific searches in small datasets. However, they lack semantic understanding and struggle with scalability and maintainability for complex patterns.\n",
    "\n",
    "In contrast, full-text search excels in handling large datasets with advanced indexing and relevance scoring, providing more semantic context and supporting complex queries. Nonetheless, it requires more setup, is resource-intensive, and may sacrifice precision for specific patterns compared to regex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c800ecf",
   "metadata": {},
   "source": [
    "### 2. Can an LLM be applied in the context of searching for documents? Justify your answer, excluding the obvious observation that an LLM can be used to formulate the answer.\n",
    "\n",
    "Yes, an LLM can enhance document search beyond simple keyword matching.\n",
    "\n",
    "LLMs improve document search by understanding context and semantics, handling complex queries, and enabling personalized, relevance-based results. They surpass traditional keyword searches by using deep language comprehension, making searches more accurate and adaptable to different domains.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
